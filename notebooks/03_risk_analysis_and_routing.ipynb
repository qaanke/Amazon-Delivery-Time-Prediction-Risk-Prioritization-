{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a7293",
   "metadata": {},
   "source": [
    "# PART 15: Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "048b8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed thresholds and weights are defined in this file.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed93993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Road and entry/exit files\n",
    "ROOT = Path(\"..\")  # notebook is defaulted under 'notebooks/'\n",
    "DATA_DIR = ROOT / \"data\" / \"processed\"\n",
    "RAW_FILE = DATA_DIR / \"delivery_cleaned.csv\"  # for coordinates and context\n",
    "\n",
    "X_TEST_CSV = DATA_DIR / \"X_test.csv\"   # _row_id, Order_ID + features\n",
    "Y_TEST_CSV = DATA_DIR / \"y_test.csv\"   # _row_id, Order_ID, y_true\n",
    "Y_PRED_CSV = DATA_DIR / \"y_pred.csv\"   # _row_id, Order_ID, y_pred\n",
    "\n",
    "OUT_RISK   = DATA_DIR / \"risk_scored.csv\"\n",
    "OUT_ROUTE  = DATA_DIR / \"smart_routing.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ba462",
   "metadata": {},
   "source": [
    "# PART 16: Loading and Merging Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fa985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(X_TEST_CSV)\n",
    "y_test = pd.read_csv(Y_TEST_CSV)\n",
    "y_pred = pd.read_csv(Y_PRED_CSV)\n",
    "\n",
    "# Are the expected identity columns available?\n",
    "for col in [\"_row_id\", \"Order_ID\"]:\n",
    "    if col not in X_test.columns:\n",
    "        raise KeyError(f\"Expecting '{col}' inside X_test..\")\n",
    "    if col not in y_test.columns:\n",
    "        raise KeyError(f\"Expecting '{col}' inside y_test.\")\n",
    "    if col not in y_pred.columns:\n",
    "        raise KeyError(f\"Expecting '{col}' inside y_pred.\")\n",
    "        \n",
    "# Merge\n",
    "df = (\n",
    "    X_test.merge(y_test[[\"_row_id\", \"Order_ID\", \"y_true\"]], on=[\"_row_id\", \"Order_ID\"], how=\"left\")\n",
    "         .merge(y_pred[[\"_row_id\", \"Order_ID\", \"y_pred\"]], on=[\"_row_id\", \"Order_ID\"], how=\"left\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4400cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Extract coordinate and contextual fields from the raw file (if any)\n",
    "raw_cols_needed = [\n",
    "    \"Store_Latitude\",\"Store_Longitude\",\"Drop_Latitude\",\"Drop_Longitude\",\n",
    "    \"Vehicle\",\"Traffic\",\"Order_Period\",\"Order_Hour\",\"Area\",\"Category\"\n",
    "]\n",
    "raw_cols_present = []\n",
    "try:\n",
    "    raw_df = pd.read_csv(RAW_FILE)\n",
    "    # If _row_id does not exist in delivery_cleaned.csv, add it.\n",
    "    if \"_row_id\" not in raw_df.columns:\n",
    "        raw_df = raw_df.reset_index().rename(columns={\"index\": \"_row_id\"})\n",
    "    raw_cols_present = [\"_row_id\"] + [c for c in raw_cols_needed if c in raw_df.columns]\n",
    "    if raw_cols_present:\n",
    "        df = df.merge(raw_df[raw_cols_present], on=\"_row_id\", how=\"left\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: delivery_cleaned.csv not found; coordinate/context fields will not be added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a415718",
   "metadata": {},
   "source": [
    "# PART 17: Errors and Risk Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a60e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error metrics\n",
    "if df[\"y_true\"].isna().any() or df[\"y_pred\"].isna().any():\n",
    "    raise ValueError(\"There is a NaN in y_true or y_pred. Check the artifact compatibility.\")\n",
    "\n",
    "df[\"error\"] = df[\"y_true\"] - df[\"y_pred\"]\n",
    "df[\"abs_error\"] = df[\"error\"].abs()\n",
    "\n",
    "# Risk threshold method: fixed threshold (in minutes)\n",
    "# We can update the following constants according to the operation/SLA if desired.\n",
    "LOW_MAX   = 15   # 0-15: low\n",
    "MED_MAX   = 30   # 15-30: medium\n",
    "# 30+: high\n",
    "\n",
    "def label_risk(abs_err: float) -> str:\n",
    "    if abs_err <= LOW_MAX:\n",
    "        return \"low\"\n",
    "    elif abs_err <= MED_MAX:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "df[\"risk_level\"] = df[\"abs_error\"].apply(label_risk)\n",
    "\n",
    "# Optional: Data-driven dynamic thresholds (IQR/percentile)\n",
    "# Q50, Q75, Q90, etc. Step 10 is disabled because it is out of scope.\n",
    "# pct50, pct75, pct90 = np.percentile(df[\"abs_error\"], [50, 75, 90])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205e766",
   "metadata": {},
   "source": [
    "# PART 18: Priority Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75788409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor maps\n",
    "risk_map = {\"low\": 1.0, \"medium\": 2.0, \"high\": 3.0}\n",
    "traffic_map = {\n",
    "    \"Low\": 1.0, \"Medium\": 2.0, \"High\": 3.0,\n",
    "    \"low\": 1.0, \"medium\": 2.0, \"high\": 3.0\n",
    "}\n",
    "vehicle_map = {\n",
    "    # Faster vehicles can have a lower coefficient (balancing risk).\n",
    "    \"motorcycle\": 1.0,\n",
    "    \"bicycle\": 1.4,\n",
    "    \"car\": 1.1,\n",
    "    \"scooter\": 1.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e209d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak hour factor: According to Order_Period\n",
    "def peak_factor(period: str) -> float:\n",
    "    if pd.isna(period):\n",
    "        return 1.0\n",
    "    p = str(period).lower()\n",
    "    if any(k in p for k in [\"lunch\", \"evening\", \"rush\", \"peak\"]):\n",
    "        return 2.0\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54c21181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights (No Step 10; fixed values)\n",
    "W_RISK    = 0.50\n",
    "W_TRAFFIC = 0.25\n",
    "W_PEAK    = 0.15\n",
    "W_VEHICLE = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eba4ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping columns (apply default if none exist)\n",
    "df[\"risk_factor\"] = df[\"risk_level\"].map(risk_map).fillna(1.0)\n",
    "if \"Traffic\" in df.columns:\n",
    "    df[\"traffic_factor\"] = df[\"Traffic\"].map(traffic_map).fillna(1.0)\n",
    "else:\n",
    "    df[\"traffic_factor\"] = 1.0\n",
    "\n",
    "if \"Vehicle\" in df.columns:\n",
    "    df[\"vehicle_factor\"] = df[\"Vehicle\"].str.lower().map(vehicle_map).fillna(1.2)\n",
    "else:\n",
    "    df[\"vehicle_factor\"] = 1.2\n",
    "\n",
    "if \"Order_Period\" in df.columns:\n",
    "    df[\"peak_factor\"] = df[\"Order_Period\"].apply(peak_factor)\n",
    "else:\n",
    "    df[\"peak_factor\"] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4bd110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score\n",
    "cols_needed = [\"risk_factor\",\"traffic_factor\",\"peak_factor\",\"vehicle_factor\"]\n",
    "if df[cols_needed].isna().any().any():\n",
    "    raise ValueError(\"NaN was found in the priority score; check the factor columns.\")\n",
    "\n",
    "df[\"priority_score\"] = (\n",
    "    W_RISK*df[\"risk_factor\"] +\n",
    "    W_TRAFFIC*df[\"traffic_factor\"] +\n",
    "    W_PEAK*df[\"peak_factor\"] +\n",
    "    W_VEHICLE*df[\"vehicle_factor\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d329be",
   "metadata": {},
   "source": [
    "# PART 19: Clustering & intracluster sorting â€” default OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a38216d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_CLUSTERING = False  # If I set it to True, a sample flow will run with KMeans.\n",
    "\n",
    "if ENABLE_CLUSTERING:\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    # Are the coordinates available?\n",
    "    for c in [\"Drop_Latitude\",\"Drop_Longitude\"]:\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(\"The following coordinates are required for clustering: 'Drop_Latitude' and 'Drop_Longitude'.\")\n",
    "\n",
    "    coords = df[[\"Drop_Latitude\",\"Drop_Longitude\"]].values\n",
    "    n = len(df)\n",
    "    k = max(2, min(15, round(n/50)))  # a crude intuition\n",
    "\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    df[\"cluster_id\"] = km.fit_predict(coords)\n",
    "\n",
    "    # Simple nearest neighbor order within a set (example)\n",
    "    seq_list = np.zeros(n, dtype=int)\n",
    "    for cid, grp in df.groupby(\"cluster_id\"):\n",
    "        idx = grp.index.values\n",
    "        if len(idx) <= 1:\n",
    "            seq_list[idx] = 1\n",
    "            continue\n",
    "        sub = grp[[\"Drop_Latitude\",\"Drop_Longitude\"]].values\n",
    "        # Starting point: the point closest to the cluster center\n",
    "        center = sub.mean(axis=0, keepdims=True)\n",
    "        start = np.argmin(((sub - center)**2).sum(axis=1))\n",
    "        # Greedy NN\n",
    "        visited = [start]\n",
    "        remain = set(range(len(sub))) - set(visited)\n",
    "        while remain:\n",
    "            last = visited[-1]\n",
    "            dists = ((sub[list(remain)] - sub[last])**2).sum(axis=1)\n",
    "            nxt_local = list(remain)[int(np.argmin(dists))]\n",
    "            visited.append(nxt_local)\n",
    "            remain.remove(nxt_local)\n",
    "        # Assign the rows\n",
    "        for order, local_i in enumerate(visited, start=1):\n",
    "            seq_list[idx[local_i]] = order\n",
    "    df[\"seq_in_cluster\"] = seq_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ff89d",
   "metadata": {},
   "source": [
    "# PART 20: Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "006549a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# risk_scored.csv: error and risk-focused summary\n",
    "risk_cols = [\n",
    "    \"_row_id\",\"Order_ID\",\"y_true\",\"y_pred\",\"error\",\"abs_error\",\"risk_level\",\n",
    "    \"Traffic\",\"Vehicle\",\"Order_Period\",\"Order_Hour\",\"Area\",\"Category\"\n",
    "]\n",
    "risk_cols = [c for c in risk_cols if c in df.columns]\n",
    "\n",
    "df[risk_cols].to_csv(OUT_RISK, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c469048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smart_routing.csv: priority and (if applicable) coordinates/cluster information\n",
    "route_cols = [\n",
    "    \"_row_id\",\"Order_ID\",\"y_pred\",\"risk_level\",\"priority_score\",\n",
    "    \"Traffic\",\"Vehicle\",\"Order_Period\",\"Order_Hour\",\"Area\",\"Category\",\n",
    "    \"Store_Latitude\",\"Store_Longitude\",\"Drop_Latitude\",\"Drop_Longitude\",\n",
    "    \"cluster_id\",\"seq_in_cluster\"\n",
    "]\n",
    "route_cols = [c for c in route_cols if c in df.columns]\n",
    "\n",
    "df[route_cols].to_csv(OUT_ROUTE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6607405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "        count\n",
      "risk         \n",
      "low      4456\n",
      "medium   2773\n",
      "high     1474\n",
      "\n",
      "priority_score examples:\n",
      "        Order_ID risk_level  priority_score\n",
      "0  qjzw184742800        low            1.17\n",
      "1  zgdu330581471     medium            1.67\n",
      "2  geop858602748        low            1.17\n",
      "3  psqt401935008     medium            1.67\n",
      "4  ocbg529160503       high            2.17\n",
      "5  tvpc165252510     medium            1.67\n",
      "6  hzkr102840172     medium            1.67\n",
      "7  qanh567637196     medium            1.67\n",
      "8  kkni283102635       high            2.17\n",
      "9  yzod046149213       high            2.17\n"
     ]
    }
   ],
   "source": [
    "# A brief summary\n",
    "print(\"\\nSummary:\")\n",
    "print(df[\"risk_level\"].value_counts(dropna=False).rename_axis(\"risk\").to_frame(\"count\"))\n",
    "print(\"\\npriority_score examples:\")\n",
    "print(df[[\"Order_ID\",\"risk_level\",\"priority_score\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cfeff2",
   "metadata": {},
   "source": [
    "# Some Additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7388932b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr(abs_error, priority_score)= 0.882\n",
      "Traffic\n",
      "High       1.36\n",
      "Jam        1.44\n",
      "Low        1.29\n",
      "Medium     1.50\n",
      "Name: priority_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Priority-score sanity check\n",
    "print('corr(abs_error, priority_score)=', df[['abs_error','priority_score']].corr().iloc[0,1].round(3))\n",
    "print(df.groupby('Traffic')['priority_score'].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddb4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
